{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:13:43.985155Z",
     "start_time": "2019-06-13T18:13:43.930097Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss, EditedNearestNeighbours,RepeatedEditedNearestNeighbours\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, if we want to use a keras NN in our Voting Ensemble, we cannot use the native sklearn function. We need to build the ensemble by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:40:26.108167Z",
     "start_time": "2019-06-13T17:40:26.069659Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:40:28.494663Z",
     "start_time": "2019-06-13T17:40:26.533303Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "# plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "DATA_PATH = '../data/'\n",
    "\n",
    "VAL_SPLITS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:40:34.642312Z",
     "start_time": "2019-06-13T17:40:28.497334Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:40:34.701888Z",
     "start_time": "2019-06-13T17:40:34.645231Z"
    }
   },
   "outputs": [],
   "source": [
    "from plot_utils import plot_confusion_matrix\n",
    "from cv_utils import run_cv_f1\n",
    "from cv_utils import plot_cv_roc\n",
    "from cv_utils import plot_cv_roc_prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:40:34.908271Z",
     "start_time": "2019-06-13T17:40:34.705380Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Experimental: Based on LightGMB https://github.com/Microsoft/LightGBM\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the project, we will only work with the training set, that we will split again into train and validation to perform the hyperparameter tuning.\n",
    "\n",
    "We will save the test set for the final part, when we have already tuned our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:40:37.949554Z",
     "start_time": "2019-06-13T17:40:34.912676Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH,'df_train.csv'))\n",
    "df.drop(columns= df.columns[0:2],inplace=True)\n",
    "df.head()\n",
    "idx_to_feat = dict(enumerate([feat for feat in df.columns if feat is not 'Class']))\n",
    "feat_to_idx = {feat : idx for idx,feat in idx_to_feat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selector Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different algorithms are trained on different features, so we need to take this into account when preparing an ensemble. This can be done through a Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:04:50.099387Z",
     "start_time": "2019-06-13T18:04:49.944807Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:04:50.234280Z",
     "start_time": "2019-06-13T18:04:50.174908Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_features(X,list_names,feat_to_idx):\n",
    "    list_idx = [feat_to_idx[feat] for feat in list_names]\n",
    "    return X[:,list_idx]\n",
    "\n",
    "list_features = ['V1','V2']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:17:56.000169Z",
     "start_time": "2019-06-13T17:17:55.894483Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (241167, 32)\n"
     ]
    }
   ],
   "source": [
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "print('X.shape:',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:01:42.650034Z",
     "start_time": "2019-06-13T17:51:50.306853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value (Train): 0.54 ± 0.03\n",
      "F1 value (Val): 0.43 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "over_sampler = SMOTE(random_state=0, n_jobs=-1)\n",
    "clf_ = xgb.sklearn.XGBClassifier(n_jobs=-1,verbosity=0, \n",
    "                                 max_depth=5, learning_rate=0.1, \n",
    "                                 random_state=0)\n",
    "# clf = Pipeline([('scaler',scaler), ('passthrough',over_sampler),('clf_',clf_)])\n",
    "clf = make_pipeline(scaler,over_sampler,clf_)\n",
    "\n",
    "scores = cross_validate(clf,X,y,cv=cv,scoring='f1',n_jobs=-1, return_train_score=True)\n",
    "print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['train_score']),\n",
    "                np.std(scores['train_score'], ddof=1)\n",
    "            ))\n",
    "print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['test_score']),\n",
    "                np.std(scores['test_score'], ddof=1)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:07:53.028924Z",
     "start_time": "2019-06-13T18:07:40.466026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value (Train): 1.00 ± 0.00\n",
      "F1 value (Val): 0.82 ± 0.06\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "list_features = ['V9','V14','V16']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})\n",
    "scaler = StandardScaler()\n",
    "# over_sampler = SMOTEENN(random_state=0)\n",
    "clf_ = ExtraTreesClassifier(n_estimators=50,n_jobs=-1,random_state=0)\n",
    "# clf = Pipeline([('scaler',scaler), ('passthrough',over_sampler),('clf_',clf_)])\n",
    "\n",
    "clf = make_pipeline(feat_select,scaler,clf_)\n",
    "\n",
    "scores = cross_validate(clf,X,y,cv=cv,scoring='f1',n_jobs=-1, return_train_score=True)\n",
    "print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['train_score']),\n",
    "                np.std(scores['train_score'], ddof=1)\n",
    "            ))\n",
    "print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['test_score']),\n",
    "                np.std(scores['test_score'], ddof=1)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:07:15.526299Z",
     "start_time": "2019-06-13T18:06:39.971430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value (Train): 0.30 ± 0.01\n",
      "F1 value (Val): 0.22 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "list_features = ['V9','V14','V16']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})\n",
    "scaler = StandardScaler()\n",
    "over_sampler = SMOTEENN(random_state=0)\n",
    "clf_ = ExtraTreesClassifier(n_estimators=50,n_jobs=-1,random_state=0)\n",
    "# clf = Pipeline([('scaler',scaler), ('passthrough',over_sampler),('clf_',clf_)])\n",
    "\n",
    "clf = make_pipeline(feat_select,scaler,over_sampler,clf_)\n",
    "\n",
    "scores = cross_validate(clf,X,y,cv=cv,scoring='f1',n_jobs=-1, return_train_score=True)\n",
    "print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['train_score']),\n",
    "                np.std(scores['train_score'], ddof=1)\n",
    "            ))\n",
    "print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['test_score']),\n",
    "                np.std(scores['test_score'], ddof=1)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:08:35.107103Z",
     "start_time": "2019-06-13T18:08:00.614745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value (Train): 1.00 ± 0.00\n",
      "F1 value (Val): 0.31 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "list_features = ['V9','V14','V16']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})\n",
    "scaler = StandardScaler()\n",
    "over_sampler = SMOTE(random_state=0, n_jobs=-1)\n",
    "clf_ = ExtraTreesClassifier(n_estimators=50,n_jobs=-1,random_state=0)\n",
    "# clf = Pipeline([('scaler',scaler), ('passthrough',over_sampler),('clf_',clf_)])\n",
    "\n",
    "clf = make_pipeline(feat_select,scaler,over_sampler,clf_)\n",
    "\n",
    "scores = cross_validate(clf,X,y,cv=cv,scoring='f1',n_jobs=-1, return_train_score=True)\n",
    "print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['train_score']),\n",
    "                np.std(scores['train_score'], ddof=1)\n",
    "            ))\n",
    "print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['test_score']),\n",
    "                np.std(scores['test_score'], ddof=1)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:11:25.977841Z",
     "start_time": "2019-06-13T18:10:44.425249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value (Train): 0.96 ± 0.01\n",
      "F1 value (Val): 0.30 ± 0.02\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "list_features = ['V9','V14','V16']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})\n",
    "scaler = StandardScaler()\n",
    "over_sampler = SMOTETomek(random_state=0)\n",
    "clf_ = ExtraTreesClassifier(n_estimators=50,n_jobs=-1,random_state=0)\n",
    "# clf = Pipeline([('scaler',scaler), ('passthrough',over_sampler),('clf_',clf_)])\n",
    "\n",
    "clf = make_pipeline(feat_select,scaler,over_sampler,clf_)\n",
    "\n",
    "scores = cross_validate(clf,X,y,cv=cv,scoring='f1',n_jobs=-1, return_train_score=True)\n",
    "print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['train_score']),\n",
    "                np.std(scores['train_score'], ddof=1)\n",
    "            ))\n",
    "print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['test_score']),\n",
    "                np.std(scores['test_score'], ddof=1)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:12:49.059625Z",
     "start_time": "2019-06-13T18:12:44.311320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value (Train): 0.00 ± 0.00\n",
      "F1 value (Val): 0.00 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "list_features = ['V3','V4','V12','V14','V16','V17']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})\n",
    "scaler = StandardScaler()\n",
    "over_sampler = NearMiss(random_state=0, n_jobs=-1)\n",
    "clf_ = ExtraTreesClassifier(n_estimators=50,n_jobs=-1,random_state=0)\n",
    "# clf = Pipeline([('scaler',scaler), ('passthrough',over_sampler),('clf_',clf_)])\n",
    "\n",
    "clf = make_pipeline(feat_select,scaler,over_sampler,clf_)\n",
    "\n",
    "scores = cross_validate(clf,X,y,cv=cv,scoring='f1',n_jobs=-1, return_train_score=True)\n",
    "print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['train_score']),\n",
    "                np.std(scores['train_score'], ddof=1)\n",
    "            ))\n",
    "print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['test_score']),\n",
    "                np.std(scores['test_score'], ddof=1)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:12:38.562377Z",
     "start_time": "2019-06-13T18:12:20.723709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value (Train): 0.96 ± 0.01\n",
      "F1 value (Val): 0.82 ± 0.05\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "list_features = ['V9','V14','V16']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})\n",
    "scaler = StandardScaler()\n",
    "over_sampler = EditedNearestNeighbours(random_state=0, n_jobs=-1)\n",
    "clf_ = ExtraTreesClassifier(n_estimators=50,n_jobs=-1,random_state=0)\n",
    "# clf = Pipeline([('scaler',scaler), ('passthrough',over_sampler),('clf_',clf_)])\n",
    "\n",
    "clf = make_pipeline(feat_select,scaler,over_sampler,clf_)\n",
    "\n",
    "scores = cross_validate(clf,X,y,cv=cv,scoring='f1',n_jobs=-1, return_train_score=True)\n",
    "print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['train_score']),\n",
    "                np.std(scores['train_score'], ddof=1)\n",
    "            ))\n",
    "print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['test_score']),\n",
    "                np.std(scores['test_score'], ddof=1)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:14:34.721656Z",
     "start_time": "2019-06-13T18:13:52.483386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value (Train): 0.95 ± 0.00\n",
      "F1 value (Val): 0.81 ± 0.04\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "list_features = ['V9','V14','V16']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create the samplers\n",
    "enn = EditedNearestNeighbours()\n",
    "renn = RepeatedEditedNearestNeighbours()\n",
    "\n",
    "clf_ = ExtraTreesClassifier(n_estimators=50,n_jobs=-1,random_state=0)\n",
    "# clf = Pipeline([('scaler',scaler), ('passthrough',over_sampler),('clf_',clf_)])\n",
    "\n",
    "clf = make_pipeline(feat_select,scaler,enn,renn,clf_)\n",
    "\n",
    "scores = cross_validate(clf,X,y,cv=cv,scoring='f1',n_jobs=-1, return_train_score=True)\n",
    "print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['train_score']),\n",
    "                np.std(scores['train_score'], ddof=1)\n",
    "            ))\n",
    "print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['test_score']),\n",
    "                np.std(scores['test_score'], ddof=1)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:15:29.272353Z",
     "start_time": "2019-06-13T18:14:53.583086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value (Train): 0.95 ± 0.01\n",
      "F1 value (Val): 0.80 ± 0.04\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "list_features = ['V9','V14','V16']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create the samplers\n",
    "enn = EditedNearestNeighbours()\n",
    "renn = RepeatedEditedNearestNeighbours()\n",
    "\n",
    "clf_ = ExtraTreesClassifier(n_estimators=50,n_jobs=-1,random_state=0)\n",
    "# clf = Pipeline([('scaler',scaler), ('passthrough',over_sampler),('clf_',clf_)])\n",
    "\n",
    "clf = make_pipeline(feat_select,enn,renn,scaler,clf_)\n",
    "\n",
    "scores = cross_validate(clf,X,y,cv=cv,scoring='f1',n_jobs=-1, return_train_score=True)\n",
    "print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['train_score']),\n",
    "                np.std(scores['train_score'], ddof=1)\n",
    "            ))\n",
    "print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['test_score']),\n",
    "                np.std(scores['test_score'], ddof=1)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T18:16:02.982761Z",
     "start_time": "2019-06-13T18:15:51.134944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 value (Train): 1.00 ± 0.00\n",
      "F1 value (Val): 0.82 ± 0.06\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "list_features = ['V9','V14','V16']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})\n",
    "scaler = StandardScaler()\n",
    "over_sampler = EditedNearestNeighbours(random_state=0, n_jobs=-1)\n",
    "clf_ = ExtraTreesClassifier(n_estimators=50,n_jobs=-1,random_state=0)\n",
    "# clf = Pipeline([('scaler',scaler), ('passthrough',over_sampler),('clf_',clf_)])\n",
    "\n",
    "clf = make_pipeline(feat_select,scaler,clf_)\n",
    "\n",
    "scores = cross_validate(clf,X,y,cv=cv,scoring='f1',n_jobs=-1, return_train_score=True)\n",
    "print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['train_score']),\n",
    "                np.std(scores['train_score'], ddof=1)\n",
    "            ))\n",
    "print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(scores['test_score']),\n",
    "                np.std(scores['test_score'], ddof=1)\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble by hand (Hard voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:18:09.587404Z",
     "start_time": "2019-06-13T16:18:09.522638Z"
    }
   },
   "outputs": [],
   "source": [
    "def hard_vote_predict(estimators, X, weights = None):\n",
    "    \"\"\"\n",
    "    Combine a dictionary of estimators to create a hard voting ensemble.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(estimators))\n",
    "    else:\n",
    "        weights = np.array(weights)\n",
    "    weights = weights.reshape((-1,1))\n",
    "    y_preds = []    \n",
    "    for name,clf in estimators.items():\n",
    "        y_pred = clf.predict(X)\n",
    "        if name is 'nn':\n",
    "            y_pred = (1*(y_pred>0.5)).reshape((-1))\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_final = 1*(np.mean(weights*y_preds,axis=0) > 0.5)\n",
    "    return y_final  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:18:10.390940Z",
     "start_time": "2019-06-13T16:18:10.332210Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "def create_clf(input_dim):\n",
    "    clf1 = Sequential([\n",
    "        Dense(8, input_shape=(input_dim,)),\n",
    "        LeakyReLU(),\n",
    "        Dense(4),\n",
    "        LeakyReLU(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='clf')\n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:18:43.711997Z",
     "start_time": "2019-06-13T16:18:43.557891Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "clf = LogisticRegression(solver='sag',random_state=0,n_jobs=-1)\n",
    "\n",
    "# In case we want to select a subset of features\n",
    "df_ = df[['Class','V3','V4','V12','V14','V16','V17']]\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "INPUT_DIM = X.shape[1]\n",
    "\n",
    "clf1 = create_clf(INPUT_DIM)\n",
    "clf1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy')\n",
    "# clf2 = RandomForestClassifier(n_estimators=100,\n",
    "#                               max_depth=6,\n",
    "#                               random_state=0,n_jobs=-1, max_features=6)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf3 = xgb.sklearn.XGBClassifier(n_jobs=-1,max_depth=5, random_state=0)\n",
    "# clf3 = LogisticRegression(n_jobs=-1)\n",
    "sklearn_clfs = [clf2,clf3]\n",
    "clfs = [clf1]+sklearn_clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:23:35.514942Z",
     "start_time": "2019-06-13T16:18:45.457242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 has ended!\n",
      "Fold 2 has ended!\n",
      "Fold 3 has ended!\n",
      "Fold 4 has ended!\n",
      "Metric value validation(va): 0.82 +- 0.05\n",
      "Metric value train: 0.88 +- 0.00\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "accuracy = []\n",
    "precision = []\n",
    "metrics_train = []\n",
    "accuracy_train = []\n",
    "precision_train = []\n",
    "\n",
    "for i, (idx_t, idx_v) in enumerate(cv.split(X,y)):\n",
    "    X_train = X[idx_t]\n",
    "    y_train = y[idx_t]\n",
    "    X_val = X[idx_v]\n",
    "    y_val = y[idx_v]\n",
    "    #Devuelve cuatro vectrores de dos elementos, el primero con los indices de train y el segundo con \n",
    "    #los de validacion \n",
    "    \n",
    "    clf1.fit(X_train,y_train,batch_size=512,epochs=50,verbose=0)\n",
    "    for clf_ in sklearn_clfs:\n",
    "        clf_.fit(X_train,y_train)\n",
    "    \n",
    "    estimators = dict(zip(['nn','rf','knn'],clfs))\n",
    "    y_pred = hard_vote_predict(estimators,X_val)\n",
    "\n",
    "\n",
    "    acc_va = accuracy_score(y_val, y_pred)\n",
    "    pre_va = precision_score(y_val, y_pred)\n",
    "#     error_va = mean_squared_error(y_val, y_pred)\n",
    "    f1_va = f1_score(y_val, y_pred)\n",
    "    #print('Recall:', acc)\n",
    "    #print('Precision:', pre)\n",
    "    #print('Error cuadratico medio:', error)\n",
    "    \n",
    "    y_pred_train = hard_vote_predict(estimators,X_train)\n",
    "\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    pre_train = precision_score(y_train, y_pred_train)\n",
    "#     error_train = mean_squared_error(y_train, y_pred_train)\n",
    "    f1_train = f1_score(y_train, y_pred_train)\n",
    "    \n",
    "    metrics.append(f1_va)\n",
    "    accuracy.append(acc_va)\n",
    "    precision.append(pre_va)\n",
    "    \n",
    "    metrics_train.append(f1_train)\n",
    "    accuracy_train.append(acc_train)\n",
    "    precision_train.append(pre_train)\n",
    "    print('Fold {} has ended!'.format(i+1))\n",
    "metric_mean = np.mean(metrics)\n",
    "metric_std = np.std(metrics, ddof = 1)\n",
    "print('Metric value validation(va): {:.2f} +- {:.2f}'.format(metric_mean,metric_std))\n",
    "#print('Mean validation: recall {:.4f} precision {:.4f}'.format(np.mean(accuracy), np.mean(precision)))\n",
    "\n",
    "\n",
    "metric_train_mean = np.mean(metrics_train)\n",
    "metric_train_std = np.std(metrics_train, ddof = 1)\n",
    "print('Metric value train: {:.2f} +- {:.2f}'.format(metric_train_mean,metric_train_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
