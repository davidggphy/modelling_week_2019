{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, if we want to use a keras NN in our Voting Ensemble, we cannot use the native sklearn function. We need to build the ensemble by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:17:50.934344Z",
     "start_time": "2019-06-13T16:17:50.872704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:17:51.120553Z",
     "start_time": "2019-06-13T16:17:51.059929Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "# plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "DATA_PATH = '../data/'\n",
    "\n",
    "VAL_SPLITS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:17:52.107693Z",
     "start_time": "2019-06-13T16:17:52.047421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:17:52.644882Z",
     "start_time": "2019-06-13T16:17:52.594738Z"
    }
   },
   "outputs": [],
   "source": [
    "from plot_utils import plot_confusion_matrix\n",
    "from cv_utils import run_cv_f1\n",
    "from cv_utils import plot_cv_roc\n",
    "from cv_utils import plot_cv_roc_prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:17:53.338690Z",
     "start_time": "2019-06-13T16:17:53.282963Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Experimental: Based on LightGMB https://github.com/Microsoft/LightGBM\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the project, we will only work with the training set, that we will split again into train and validation to perform the hyperparameter tuning.\n",
    "\n",
    "We will save the test set for the final part, when we have already tuned our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:14:19.975310Z",
     "start_time": "2019-06-13T17:14:16.870886Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH,'df_train.csv'))\n",
    "df.drop(columns= df.columns[0:2],inplace=True)\n",
    "df.head()\n",
    "idx_to_feat = dict(enumerate([feat for feat in df.columns if feat is not 'Class']))\n",
    "feat_to_idx = {feat : idx for idx,feat in idx_to_feat.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selector Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different algorithms are trained on different features, so we need to take this into account when preparing an ensemble. This can be done through a Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:24:51.246581Z",
     "start_time": "2019-06-13T17:24:51.158913Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:24:52.500310Z",
     "start_time": "2019-06-13T17:24:52.444199Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_features(X,list_names,feat_to_idx):\n",
    "    list_idx = [feat_to_idx[feat] for feat in list_names]\n",
    "    return X[:,list_idx]\n",
    "\n",
    "list_features = ['V1','V2']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:17:56.000169Z",
     "start_time": "2019-06-13T17:17:55.894483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (241167, 32)\n"
     ]
    }
   ],
   "source": [
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "print('X.shape:',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T17:25:31.483656Z",
     "start_time": "2019-06-13T17:25:31.425442Z"
    }
   },
   "outputs": [],
   "source": [
    "list_features = ['V1','V2']\n",
    "feat_select = FunctionTransformer(select_features,validate=True,\n",
    "                    kw_args={'list_names':list_features,\n",
    "                             'feat_to_idx':feat_to_idx})\n",
    "# clf = make_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble by hand (Hard voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:18:09.587404Z",
     "start_time": "2019-06-13T16:18:09.522638Z"
    }
   },
   "outputs": [],
   "source": [
    "def hard_vote_predict(estimators, X, weights = None):\n",
    "    \"\"\"\n",
    "    Combine a dictionary of estimators to create a hard voting ensemble.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(estimators))\n",
    "    else:\n",
    "        weights = np.array(weights)\n",
    "    weights = weights.reshape((-1,1))\n",
    "    y_preds = []    \n",
    "    for name,clf in estimators.items():\n",
    "        y_pred = clf.predict(X)\n",
    "        if name is 'nn':\n",
    "            y_pred = (1*(y_pred>0.5)).reshape((-1))\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_final = 1*(np.mean(weights*y_preds,axis=0) > 0.5)\n",
    "    return y_final  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:18:10.390940Z",
     "start_time": "2019-06-13T16:18:10.332210Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "def create_clf(input_dim):\n",
    "    clf1 = Sequential([\n",
    "        Dense(8, input_shape=(input_dim,)),\n",
    "        LeakyReLU(),\n",
    "        Dense(4),\n",
    "        LeakyReLU(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='clf')\n",
    "    return clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:18:43.711997Z",
     "start_time": "2019-06-13T16:18:43.557891Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "clf = LogisticRegression(solver='sag',random_state=0,n_jobs=-1)\n",
    "\n",
    "# In case we want to select a subset of features\n",
    "df_ = df[['Class','V3','V4','V12','V14','V16','V17']]\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "INPUT_DIM = X.shape[1]\n",
    "\n",
    "clf1 = create_clf(INPUT_DIM)\n",
    "clf1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy')\n",
    "# clf2 = RandomForestClassifier(n_estimators=100,\n",
    "#                               max_depth=6,\n",
    "#                               random_state=0,n_jobs=-1, max_features=6)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "clf3 = xgb.sklearn.XGBClassifier(n_jobs=-1,max_depth=5, random_state=0)\n",
    "# clf3 = LogisticRegression(n_jobs=-1)\n",
    "sklearn_clfs = [clf2,clf3]\n",
    "clfs = [clf1]+sklearn_clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T16:23:35.514942Z",
     "start_time": "2019-06-13T16:18:45.457242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 has ended!\n",
      "Fold 2 has ended!\n",
      "Fold 3 has ended!\n",
      "Fold 4 has ended!\n",
      "Metric value validation(va): 0.82 +- 0.05\n",
      "Metric value train: 0.88 +- 0.00\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "accuracy = []\n",
    "precision = []\n",
    "metrics_train = []\n",
    "accuracy_train = []\n",
    "precision_train = []\n",
    "\n",
    "for i, (idx_t, idx_v) in enumerate(cv.split(X,y)):\n",
    "    X_train = X[idx_t]\n",
    "    y_train = y[idx_t]\n",
    "    X_val = X[idx_v]\n",
    "    y_val = y[idx_v]\n",
    "    #Devuelve cuatro vectrores de dos elementos, el primero con los indices de train y el segundo con \n",
    "    #los de validacion \n",
    "    \n",
    "    clf1.fit(X_train,y_train,batch_size=512,epochs=50,verbose=0)\n",
    "    for clf_ in sklearn_clfs:\n",
    "        clf_.fit(X_train,y_train)\n",
    "    \n",
    "    estimators = dict(zip(['nn','rf','knn'],clfs))\n",
    "    y_pred = hard_vote_predict(estimators,X_val)\n",
    "\n",
    "\n",
    "    acc_va = accuracy_score(y_val, y_pred)\n",
    "    pre_va = precision_score(y_val, y_pred)\n",
    "#     error_va = mean_squared_error(y_val, y_pred)\n",
    "    f1_va = f1_score(y_val, y_pred)\n",
    "    #print('Recall:', acc)\n",
    "    #print('Precision:', pre)\n",
    "    #print('Error cuadratico medio:', error)\n",
    "    \n",
    "    y_pred_train = hard_vote_predict(estimators,X_train)\n",
    "\n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    pre_train = precision_score(y_train, y_pred_train)\n",
    "#     error_train = mean_squared_error(y_train, y_pred_train)\n",
    "    f1_train = f1_score(y_train, y_pred_train)\n",
    "    \n",
    "    metrics.append(f1_va)\n",
    "    accuracy.append(acc_va)\n",
    "    precision.append(pre_va)\n",
    "    \n",
    "    metrics_train.append(f1_train)\n",
    "    accuracy_train.append(acc_train)\n",
    "    precision_train.append(pre_train)\n",
    "    print('Fold {} has ended!'.format(i+1))\n",
    "metric_mean = np.mean(metrics)\n",
    "metric_std = np.std(metrics, ddof = 1)\n",
    "print('Metric value validation(va): {:.2f} +- {:.2f}'.format(metric_mean,metric_std))\n",
    "#print('Mean validation: recall {:.4f} precision {:.4f}'.format(np.mean(accuracy), np.mean(precision)))\n",
    "\n",
    "\n",
    "metric_train_mean = np.mean(metrics_train)\n",
    "metric_train_std = np.std(metrics_train, ddof = 1)\n",
    "print('Metric value train: {:.2f} +- {:.2f}'.format(metric_train_mean,metric_train_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
