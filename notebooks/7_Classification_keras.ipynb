{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder (Semi-supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:34:03.023260Z",
     "start_time": "2019-06-13T15:34:02.998742Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:34:11.425102Z",
     "start_time": "2019-06-13T15:34:03.493403Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:34:13.276967Z",
     "start_time": "2019-06-13T15:34:11.428217Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "# plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "VAL_SPLITS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:34:13.559014Z",
     "start_time": "2019-06-13T15:34:13.280956Z"
    }
   },
   "outputs": [],
   "source": [
    "from plot_utils import plot_confusion_matrix\n",
    "from cv_utils import run_cv_f1\n",
    "from cv_utils import plot_cv_roc\n",
    "from cv_utils import plot_cv_roc_prc\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the project, we will only work with the training set, that we will split again into train and validation to perform the hyperparameter tuning.\n",
    "\n",
    "We will save the test set for the final part, when we have already tuned our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:34:16.418963Z",
     "start_time": "2019-06-13T15:34:13.563528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>TimeScaled</th>\n",
       "      <th>TimeSin</th>\n",
       "      <th>TimeCos</th>\n",
       "      <th>AmountBC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.829392</td>\n",
       "      <td>1.118573</td>\n",
       "      <td>0.926038</td>\n",
       "      <td>1.163686</td>\n",
       "      <td>0.009824</td>\n",
       "      <td>0.527347</td>\n",
       "      <td>0.173370</td>\n",
       "      <td>0.723997</td>\n",
       "      <td>-0.638939</td>\n",
       "      <td>-0.162923</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298908</td>\n",
       "      <td>-0.060301</td>\n",
       "      <td>-0.217935</td>\n",
       "      <td>0.291312</td>\n",
       "      <td>0.120779</td>\n",
       "      <td>0</td>\n",
       "      <td>0.460069</td>\n",
       "      <td>-0.480989</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>3.195062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.814527</td>\n",
       "      <td>1.613321</td>\n",
       "      <td>0.654307</td>\n",
       "      <td>0.581821</td>\n",
       "      <td>0.399491</td>\n",
       "      <td>0.730040</td>\n",
       "      <td>0.456233</td>\n",
       "      <td>-2.464347</td>\n",
       "      <td>0.654797</td>\n",
       "      <td>2.248682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329526</td>\n",
       "      <td>-0.307374</td>\n",
       "      <td>-0.440007</td>\n",
       "      <td>-2.135657</td>\n",
       "      <td>0.011041</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266395</td>\n",
       "      <td>-0.204567</td>\n",
       "      <td>-0.978853</td>\n",
       "      <td>3.125269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.105028</td>\n",
       "      <td>-0.700400</td>\n",
       "      <td>-1.338043</td>\n",
       "      <td>-0.596395</td>\n",
       "      <td>-0.395217</td>\n",
       "      <td>-0.755050</td>\n",
       "      <td>-0.276951</td>\n",
       "      <td>-0.291562</td>\n",
       "      <td>-0.965418</td>\n",
       "      <td>1.107179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278137</td>\n",
       "      <td>-0.040685</td>\n",
       "      <td>0.789267</td>\n",
       "      <td>-0.066054</td>\n",
       "      <td>-0.069956</td>\n",
       "      <td>0</td>\n",
       "      <td>0.762303</td>\n",
       "      <td>-0.153992</td>\n",
       "      <td>-0.988072</td>\n",
       "      <td>3.421235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.205839</td>\n",
       "      <td>-1.023897</td>\n",
       "      <td>-1.270137</td>\n",
       "      <td>-0.950174</td>\n",
       "      <td>-0.868712</td>\n",
       "      <td>-0.975492</td>\n",
       "      <td>-0.475464</td>\n",
       "      <td>-0.280564</td>\n",
       "      <td>0.503713</td>\n",
       "      <td>0.448173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041177</td>\n",
       "      <td>0.089158</td>\n",
       "      <td>1.105794</td>\n",
       "      <td>-0.066285</td>\n",
       "      <td>-0.079881</td>\n",
       "      <td>0</td>\n",
       "      <td>0.879740</td>\n",
       "      <td>-0.998227</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>1.072145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.027090</td>\n",
       "      <td>-0.778666</td>\n",
       "      <td>-1.552755</td>\n",
       "      <td>-0.558679</td>\n",
       "      <td>0.020939</td>\n",
       "      <td>-0.026071</td>\n",
       "      <td>-0.207810</td>\n",
       "      <td>-0.124288</td>\n",
       "      <td>-0.635953</td>\n",
       "      <td>0.817757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033477</td>\n",
       "      <td>-0.157992</td>\n",
       "      <td>-0.606327</td>\n",
       "      <td>-0.003931</td>\n",
       "      <td>-0.039868</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821649</td>\n",
       "      <td>-0.783558</td>\n",
       "      <td>-0.621319</td>\n",
       "      <td>3.971490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -0.829392  1.118573  0.926038  1.163686  0.009824  0.527347  0.173370   \n",
       "1 -2.814527  1.613321  0.654307  0.581821  0.399491  0.730040  0.456233   \n",
       "2  2.105028 -0.700400 -1.338043 -0.596395 -0.395217 -0.755050 -0.276951   \n",
       "3  2.205839 -1.023897 -1.270137 -0.950174 -0.868712 -0.975492 -0.475464   \n",
       "4  2.027090 -0.778666 -1.552755 -0.558679  0.020939 -0.026071 -0.207810   \n",
       "\n",
       "         V8        V9       V10  ...       V24       V25       V26       V27  \\\n",
       "0  0.723997 -0.638939 -0.162923  ... -0.298908 -0.060301 -0.217935  0.291312   \n",
       "1 -2.464347  0.654797  2.248682  ... -0.329526 -0.307374 -0.440007 -2.135657   \n",
       "2 -0.291562 -0.965418  1.107179  ... -0.278137 -0.040685  0.789267 -0.066054   \n",
       "3 -0.280564  0.503713  0.448173  ... -0.041177  0.089158  1.105794 -0.066285   \n",
       "4 -0.124288 -0.635953  0.817757  ...  0.033477 -0.157992 -0.606327 -0.003931   \n",
       "\n",
       "        V28  Class  TimeScaled   TimeSin   TimeCos  AmountBC  \n",
       "0  0.120779      0    0.460069 -0.480989  0.876727  3.195062  \n",
       "1  0.011041      0    0.266395 -0.204567 -0.978853  3.125269  \n",
       "2 -0.069956      0    0.762303 -0.153992 -0.988072  3.421235  \n",
       "3 -0.079881      0    0.879740 -0.998227  0.059524  1.072145  \n",
       "4 -0.039868      0    0.821649 -0.783558 -0.621319  3.971490  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH,'df_train.csv'))\n",
    "df.drop(columns= df.columns[0:2],inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are always using cross validation with `VAL_SPLITS` folds, (in general, 4), here we are gonna set only one split in order to explore how the Autoencoder works and get intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:37:34.133475Z",
     "start_time": "2019-06-13T15:37:34.079136Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LeakyReLU\n",
    "\n",
    "\n",
    "def create_clf(input_dim):\n",
    "    clf = Sequential([\n",
    "        Dense(40, input_shape=(input_dim,)),\n",
    "        LeakyReLU(),\n",
    "        Dense(16),\n",
    "        LeakyReLU(),\n",
    "        Dense(16),\n",
    "        LeakyReLU(),\n",
    "        Dense(8),\n",
    "        LeakyReLU(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='clf')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:37:35.754841Z",
     "start_time": "2019-06-13T15:37:35.650491Z"
    }
   },
   "outputs": [],
   "source": [
    "# In case we want to select a subset of features\n",
    "# df_ = df[['Class','V9','V14','V16','V2','V3','V17']]\n",
    "df_ = df\n",
    "X = df_.drop(columns='Class').to_numpy()\n",
    "y = df_['Class'].to_numpy()\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "ENCODED_DIM = 2\n",
    "INPUT_DIM = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:56:46.184575Z",
     "start_time": "2019-06-13T15:56:46.118374Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def run_cv_f1_nn(create_clf, cv, X, y, calculate_on_train=True, verbose=True, save_models = False):\n",
    "    # We create two eampty lists to save the metrics at each fold for train\n",
    "    # and validation.\n",
    "    metrics = []\n",
    "    if calculate_on_train:\n",
    "        metrics_train = []\n",
    "    # Loop over the different validation folds\n",
    "    val_iterable = cv.split(X, y)\n",
    "    for i, (idx_t, idx_v) in enumerate(val_iterable):\n",
    "        X_train = X[idx_t]\n",
    "        y_train = y[idx_t]\n",
    "        X_val = X[idx_v]\n",
    "        y_val = y[idx_v]\n",
    "        \n",
    "        clf = create_clf(INPUT_DIM)\n",
    "        clf.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy')\n",
    "        clf.fit(X_train, y_train,batch_size=512, epochs=50,shuffle=True,verbose=0)\n",
    "\n",
    "        y_pred = clf.predict(X_val)\n",
    "        y_pred = 1*(y_pred>0.5)\n",
    "        metric = f1_score(y_val, y_pred)\n",
    "        metrics.append(metric)\n",
    "        if calculate_on_train:\n",
    "            y_t_pred = clf.predict(X_train)\n",
    "            y_t_pred = 1*(y_t_pred>0.5)\n",
    "            metric_train = f1_score(y_train, y_t_pred)\n",
    "            metrics_train.append(metric_train)\n",
    "        if verbose:\n",
    "            print('{}-fold / {} completed!'.format(i + 1,\n",
    "                                                   cv.get_n_splits()))\n",
    "        if save_models:\n",
    "            # Save the models into files for future use\n",
    "            clf.save('models_nn_clf/clf_nn_fold_'+str(i+1)+'.h5')\n",
    "    \n",
    "    if calculate_on_train:\n",
    "        if verbose:\n",
    "            print('F1 value (Train): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(metrics_train),\n",
    "                np.std(metrics_train, ddof=1)\n",
    "            ))\n",
    "            print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(metrics),\n",
    "                np.std(metrics, ddof=1)\n",
    "            ))\n",
    "        return metrics, metrics_train\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('F1 value (Val): {:.2f} ± {:.2f}'.format(\n",
    "                np.mean(metrics),\n",
    "                np.std(metrics, ddof=1)\n",
    "            ))\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:41:03.159962Z",
     "start_time": "2019-06-13T15:38:36.235855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-fold / 4 completed!\n",
      "2-fold / 4 completed!\n",
      "3-fold / 4 completed!\n",
      "4-fold / 4 completed!\n",
      "F1 value (Train): 0.93 ± 0.02\n",
      "F1 value (Val): 0.80 ± 0.07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7454545454545455,\n",
       "  0.7317073170731706,\n",
       "  0.8793103448275861,\n",
       "  0.8363636363636363],\n",
       " [0.9590288315629741,\n",
       "  0.9165446559297219,\n",
       "  0.9183359013867489,\n",
       "  0.9256965944272445])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "run_cv_f1_nn(create_clf,cv,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:45:51.463687Z",
     "start_time": "2019-06-13T15:43:22.271600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-fold / 4 completed!\n",
      "2-fold / 4 completed!\n",
      "3-fold / 4 completed!\n",
      "4-fold / 4 completed!\n",
      "F1 value (Train): 0.91 ± 0.03\n",
      "F1 value (Val): 0.79 ± 0.07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7166666666666667,\n",
       "  0.7592592592592592,\n",
       "  0.8518518518518519,\n",
       "  0.8468468468468469],\n",
       " [0.9219219219219219,\n",
       "  0.9337442218798152,\n",
       "  0.8709677419354839,\n",
       "  0.9073783359497646])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_clf(input_dim):\n",
    "    clf = Sequential([\n",
    "        Dense(32, input_shape=(input_dim,)),\n",
    "        LeakyReLU(),\n",
    "        Dense(16),\n",
    "        LeakyReLU(),\n",
    "        Dense(8),\n",
    "        LeakyReLU(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='clf')\n",
    "    return clf\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "run_cv_f1_nn(create_clf,cv,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:59:55.371222Z",
     "start_time": "2019-06-13T15:56:48.229841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-fold / 4 completed!\n",
      "2-fold / 4 completed!\n",
      "3-fold / 4 completed!\n",
      "4-fold / 4 completed!\n",
      "F1 value (Train): 0.84 ± 0.03\n",
      "F1 value (Val): 0.81 ± 0.05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.8, 0.743362831858407, 0.8596491228070176, 0.8363636363636363],\n",
       " [0.8717156105100464,\n",
       "  0.8502269288956128,\n",
       "  0.8112324492979719,\n",
       "  0.8291873963515755])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_clf(input_dim):\n",
    "    clf = Sequential([\n",
    "        Dense(8, input_shape=(input_dim,)),\n",
    "        LeakyReLU(),\n",
    "        Dense(4),\n",
    "        LeakyReLU(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='clf')\n",
    "    return clf\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "run_cv_f1_nn(create_clf,cv,X,y, save_models=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-13T15:51:03.693410Z",
     "start_time": "2019-06-13T15:48:32.696195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-fold / 4 completed!\n",
      "2-fold / 4 completed!\n",
      "3-fold / 4 completed!\n",
      "4-fold / 4 completed!\n",
      "F1 value (Train): 0.84 ± 0.02\n",
      "F1 value (Val): 0.78 ± 0.06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7272727272727272,\n",
       "  0.7289719626168223,\n",
       "  0.8360655737704918,\n",
       "  0.8363636363636363],\n",
       " [0.8515497553017944,\n",
       "  0.8055987558320373,\n",
       "  0.8522550544323484,\n",
       "  0.848780487804878])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_clf(input_dim):\n",
    "    clf = Sequential([\n",
    "        Dense(7, input_shape=(input_dim,)),\n",
    "        LeakyReLU(),\n",
    "        Dense(5),\n",
    "        LeakyReLU(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='clf')\n",
    "    return clf\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=VAL_SPLITS,test_size=0.15,random_state=0)\n",
    "run_cv_f1_nn(create_clf,cv,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
